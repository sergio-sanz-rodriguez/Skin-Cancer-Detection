{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8df50-5083-46f5-bd51-8a341d9b47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume df is your original DataFrame with 'target' as the label column\n",
    "\n",
    "# Split the dataset into two subsets based on the target label\n",
    "df_class_0 = df[df['target'] == 0]\n",
    "df_class_1 = df[df['target'] == 1]\n",
    "\n",
    "# Perform EDA separately on df_class_0 and df_class_1 here\n",
    "\n",
    "# After EDA, merge the subsets back together, with class 0 data first\n",
    "df_merged = pd.concat([df_class_0, df_class_1])\n",
    "\n",
    "# Extract features and labels\n",
    "X = df_merged.drop(columns=['target'])\n",
    "y = df_merged['target']\n",
    "\n",
    "# Perform a stratified train-test split to maintain the class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Check the class distribution in the train and test sets\n",
    "print(f\"Training set class distribution: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Test set class distribution: {y_test.value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b470c-b69f-4058-b865-fe3ae69def1a",
   "metadata": {},
   "source": [
    "## Some links of interests\n",
    "\n",
    "### MFEUsLNet\n",
    "https://www.sciencedirect.com/science/article/pii/S2215098624000181#b0155  \n",
    "\n",
    "From the state of the art:\n",
    "\n",
    "Ghosh et al. [31] proposed **SkinNet-16**, a deep-learning model aimed at distinguishing between benign and malignant skin lesions. Their approach employs advanced neural network architecture to enhance classification accuracy.\n",
    "\n",
    "*Pronab Ghosh, Sami Azam, Ryana Quadir, Asif Karim, F.M. Shamrat, Shohag Kumar Bhowmik, Mirjam Jonkman, Khan Md Hasib, Kawsar Ahmed, SkinNet-16: a deep learning approach to identify benign and malignant skin lesions, Front. Oncol. 12 (2022) 931141.*\n",
    "\n",
    "### EfficientNet V2 B0\n",
    "https://www.kaggle.com/code/matthewjansen/transfer-learning-skin-cancer-classification  \n",
    "\n",
    "### Inception-RestNet-v2\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9759648/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc5872-dfcb-4fa1-904e-dff5f161c227",
   "metadata": {},
   "source": [
    "### Sample Code for using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f95c0e-c87f-427f-a859-c51080bffe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load example dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Suggest values for the hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None])\n",
    "    \n",
    "    # Create the model with these hyperparameters\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    preds = model.predict(X_val)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, preds)\n",
    "    \n",
    "    # We want to maximize accuracy, so return it as a negative value\n",
    "    return accuracy\n",
    "\n",
    "# Create a study object and specify the direction of optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Start the optimization\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)  # n_trials is the number of trials, n_jobs=-1 uses all CPUs\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f'Best trial number: {best_trial.number}')\n",
    "print(f'Best value (accuracy): {best_trial.value}')\n",
    "print(f'Best hyperparameters: {best_trial.params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eedbc2-8827-416c-9a23-6a71a7790d94",
   "metadata": {},
   "source": [
    "# XGBoost imbalance classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11393e-a1dc-47ec-8e22-fa91026ec587",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/xgboost-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcdc35-f82c-4a7f-9dbd-ac67d0b59f67",
   "metadata": {},
   "source": [
    "# Enabling GPUs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96f71a-723d-410d-8e83-da25c14cf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability (before building the model)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462111b-0efe-45eb-a6c3-98ca0a03cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Specify GPU Device\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# List available devices\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs: \", devices)\n",
    "\n",
    "# Set memory growth to avoid allocating all GPU memory upfront\n",
    "for device in devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "# Specify a specific GPU device to use (if you have multiple GPUs)\n",
    "with tf.device('/GPU:0'):  # or '/GPU:1', etc.\n",
    "    # Your model code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d816005-50a7-40ac-b0ca-4222edf59876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set GPU Device Visibility\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8dfe8-de4a-4ec4-93be-2ebaa08d66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Limit GPU Memory Usage\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# List available devices\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs: \", devices)\n",
    "\n",
    "for device in devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    # Or set a specific memory limit if needed\n",
    "    # tf.config.experimental.set_virtual_device_configuration(\n",
    "    #     device,\n",
    "    #     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c85e39-0cc0-465c-b7fd-db50d786c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Check Device Placement\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    # Define and compile your model here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1bebdd-44b8-4409-9f3e-e58de8131d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "# Set environment variable to use only the first GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295783fe-36bb-40e4-960c-491a0ebfbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Forcing GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57558dd-b508-4a30-9adf-7ba50f44db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "# Set environment variable to use only the first GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Optionally, set memory growth to avoid TensorFlow allocating all GPU memory upfront\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Code below to build the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99323ec-e4fc-45e8-98b1-e10cd9f785be",
   "metadata": {},
   "source": [
    "# Steps to design a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b73f1-d8be-4ec8-aa49-03bff83d0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare data\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'feature1': [1.2, 2.3, 3.1, 4.5],\n",
    "    'feature2': [5.1, 3.3, 6.2, 1.9],\n",
    "    'feature3': [7.1, 8.3, 9.4, 10.2],\n",
    "    'label': [0, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c4be6-634f-4b64-82b0-89f274982a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create TensorFlow Datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "# Convert DataFrame to TensorFlow dataset\n",
    "def df_to_dataset(dataframe, labels, shuffle=True, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe.values, labels.values))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataframe))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Split into training and validation sets (e.g., 80/20 split)\n",
    "train_size = int(0.8 * len(X))\n",
    "train_X, val_X = X[:train_size], X[train_size:]\n",
    "train_y, val_y = y[:train_size], y[train_size:]\n",
    "\n",
    "# Generate datasets\n",
    "train_dataset = df_to_dataset(train_X, train_y, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataset = df_to_dataset(val_X, val_y, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9d679-fb72-43a1-a894-019161e43fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Design the Neural Network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_X.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f1048-695d-45ef-821d-ae8cec9a4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train the Model\n",
    "# Checkpoint callbacks\n",
    "best_checkpoint_path = f\"../models/tabular_nn_best.keras\"\n",
    "best_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=best_checkpoint_path, save_best_only=True)\n",
    "\n",
    "final_checkpoint_path = f\"../models/tabular_nn_final.keras\"\n",
    "final_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=final_checkpoint_path)\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[best_checkpoint_callback, final_checkpoint_callback, reduce_lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffe5d6-d9b9-4a4f-ae49-9d681e857d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loading and Continuing Training\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(best_checkpoint_path)\n",
    "\n",
    "# Continue training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,  # Continue for more epochs\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[best_checkpoint_callback, final_checkpoint_callback, reduce_lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3580b2-bcca-4e33-a8c7-8ebbc312f372",
   "metadata": {},
   "source": [
    "Making your CNN model deterministic in training involves setting seeds for random number generators across various components of your machine learning environment, such as TensorFlow/Keras, NumPy, and Python. This minimizes the randomness in weight initialization, data shuffling, and GPU operations, leading to more consistent training results.\n",
    "\n",
    "Here’s how you can modify your code to make it as deterministic as possible:\n",
    "\n",
    "1. Set Seeds for TensorFlow, NumPy, and Python\n",
    "python\n",
    "Copy code\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Ensure deterministic behavior on the GPU\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "2. Control Randomness in Data Augmentation (If Applicable)\n",
    "If you are using data augmentation, ensure the operations are deterministic. For example, in Keras’ ImageDataGenerator, you can set a seed when generating augmented data:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=32, seed=SEED)\n",
    "3. Modify Your Model Initialization\n",
    "In your model architecture, you've already set a seed for the GlorotUniform initializer. Ensure you set the same seed for all the initializers if they're used in other layers.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "\n",
    "# Initialize the ResNet model\n",
    "base_model = ResNet152V2(weights='imagenet', include_top=False, input_shape=(IM_SIZE, IM_SIZE, 3))\n",
    "\n",
    "# Make all layers trainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Build the custom layers on top of ResNet\n",
    "x = GlobalAveragePooling2D()(base_model.output)    \n",
    "x = Dense(NEURONS_1, kernel_initializer=glorot_uniform(seed=SEED),  activation=EVALUATION, kernel_regularizer=tf.keras.regularizers.l2(REG_RATE))(x)\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "x = Dense(NEURONS_2, kernel_initializer=glorot_uniform(seed=SEED), activation=EVALUATION, kernel_regularizer=tf.keras.regularizers.l2(REG_RATE))(x)\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "x = Dense(NEURONS_3, kernel_initializer=glorot_uniform(seed=SEED), activation=EVALUATION, kernel_regularizer=tf.keras.regularizers.l2(REG_RATE))(x)\n",
    "x = Dropout(DROPOUT_RATE)(x)  \n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Build the model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model with the Adam optimizer\n",
    "optimizer = Adam(learning_rate=LEARN_RATE)\n",
    "eval_metrics = [\"accuracy\", AUC(from_logits=False), SpecificityAtSensitivity(sensitivity=0.8)]\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=eval_metrics)\n",
    "4. Use Deterministic Operations (for GPU)\n",
    "As TensorFlow can exhibit non-deterministic behavior on GPUs, it's important to ensure that deterministic operations are enforced:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "# Set the environment variable to enforce deterministic operations\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "5. Limitations\n",
    "GPU Operations: Even with deterministic operations enabled, some GPU operations may still introduce non-determinism due to the nature of parallel computation. You may need to run on a CPU to get fully deterministic results, though this may significantly slow down training.\n",
    "Dropout and Batch Normalization: Dropout and Batch Normalization layers can introduce variability due to their nature. Setting seeds and ensuring deterministic behavior can mitigate, but not completely eliminate, this variability.\n",
    "6. Potential Post-Training Solution\n",
    "If full determinism isn't feasible, consider saving the model's weights after training and loading them for prediction to ensure consistency. This won't solve the variability during training but will guarantee that your predictions remain consistent.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Save the model weights after training\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "# Load the model weights for prediction\n",
    "model.load_weights('model_weights.h5')\n",
    "Conclusion\n",
    "By setting seeds across TensorFlow, NumPy, and Python, and enforcing deterministic operations, you can greatly reduce the non-determinism in your CNN training process. However, due to the inherent randomness in neural network training, especially on GPUs, complete determinism may not always be achievable. Saving and loading the model weights can provide consistency for predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82ea75",
   "metadata": {},
   "source": [
    "In your final Kaggle notebook submission, you can indeed include the code to train your models, but as you've noted, training a Convolutional Neural Network (CNN) might not result in the exact same model each time due to its inherent non-determinism. However, there are ways to address this issue, and you can structure your notebook accordingly:\n",
    "1. Include the Training Code:\n",
    "•\tInclude the code to train your CNN model, as this shows transparency and ensures that your approach is fully reproducible in principle.\n",
    "•\tIf you're using specific random seeds to control for some randomness (e.g., in data splitting, weight initialization), make sure to include those in your notebook.\n",
    "2. Explain the Non-Determinism:\n",
    "•\tIn the markdown cells of your notebook, clearly explain that due to the nature of neural networks and certain aspects of the training process (like GPU operations, dropout, etc.), the model might not produce the exact same results every time.\n",
    "•\tMention that while the architecture and training process are reproducible, minor variations in the trained weights might occur.\n",
    "3. Save and Load the Model (Optional but Transparent):\n",
    "•\tIf you're allowed by the competition rules, you could save the trained CNN model's weights within the notebook after training and load them later to generate predictions. This way, you ensure that your exact trained model is used for prediction, even if it's trained on-the-fly in the notebook.\n",
    "•\tExample:\n",
    "python\n",
    "Copy code\n",
    "# Save the model\n",
    "model.save('cnn_model.h5')\n",
    "\n",
    "# Later, load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('cnn_model.h5')\n",
    "•\tNote: Ensure this is done within the notebook session and doesn't rely on external files not generated within the notebook.\n",
    "4. Potential Workaround:\n",
    "•\tIf the competition allows pre-trained models from public sources, consider training your CNN model on a larger dataset or using a pre-trained model that’s publicly available (like a model pre-trained on ImageNet) and fine-tuning it. This way, you could avoid some randomness and make your approach more reproducible.\n",
    "5. Document the Process:\n",
    "•\tDocument your training procedure, including any attempts to control for randomness and explain why exact reproducibility might be challenging.\n",
    "•\tDescribe how your feature extraction works and how this feature vector is used in the downstream task.\n",
    "6. Use Model Checkpointing:\n",
    "•\tIf you have time constraints, use checkpointing in your training process to save the best model during training and then use this model to generate predictions.\n",
    "Example Workflow:\n",
    "1.\tPreprocessing and CNN training code: Include the code for preprocessing and training the CNN. Set random seeds where possible.\n",
    "2.\tModel checkpoint: Save the model weights during training.\n",
    "3.\tLoading and generating predictions: Load the saved weights and generate predictions.\n",
    "4.\tMarkdown explanations: Include markdown cells explaining the non-deterministic nature of CNNs and how you’ve handled it.\n",
    "Conclusion:\n",
    "While training a CNN can introduce non-determinism, you can mitigate this by controlling randomness where possible, saving and loading model weights within the notebook, and clearly documenting the process. Ensure that your notebook can be executed from start to finish on Kaggle's platform to generate the required submission file (submission.csv). If exact reproducibility isn't feasible, providing the code and explaining the limitations will demonstrate your good faith effort in making the process as transparent and reproducible as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc706b7-55af-45b2-900b-dcde5db731fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_common",
   "language": "python",
   "name": ".venv_common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
